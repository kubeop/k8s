---
- name: "Install dependency"
  package:
    name:
      - libnetfilter_cthelper
      - libnetfilter_cttimeout
      - libnetfilter_queue
      - conntrack-tools
      - ipvsadm
      - ipset
      - socat
    state: present
  when:
    - ansible_os_family in ['RedHat', 'AlmaLinux', 'Rocky', 'TencentOS']
  tags: worker_app

- name: "Install dependency"
  package:
    name:
      - libnetfilter-cthelper0
      - libnetfilter-cttimeout1
      - libnetfilter-queue1
      - libnetfilter-conntrack3
      - conntrack
      - ipvsadm
      - ipset
      - socat
    state: present
  when:
    - ansible_os_family in ["Debian"]
  tags: worker_app

- name: "Enabled modlue"
  modprobe:
    name: "{{ item }}"
    state: present
  with_items:
    - "ip_vs"
    - "ip_vs_rr"
    - "ip_vs_wrr"
    - "ip_vs_lc"
    - "ip_vs_wlc"
    - "ip_vs_sh"
    - "ip_vs_dh"
    - "ip_vs_sed"
    - "ip_vs_nq"
    - "bridge"
    - "overlay"
    - "ip_tables"
    - "iptable_filter"
    - "br_netfilter"
  tags: worker_mod

- name: "Enabled conntrack modlue < 4.19.0"
  modprobe:
    name: "nf_conntrack_ipv4"
    state: present
  when: ((ansible_os_family in ['RedHat', 'AlmaLinux', 'Rocky'] and ansible_distribution_major_version | int != 8) or ansible_os_family in ['Debian']) and
        ansible_kernel.split('-')[0] is version('4.19.0', '<')
  tags: worker_mod

- name: "Enabled conntrack modlue > 4.19.0"
  modprobe:
    name: "nf_conntrack"
    state: present
  when: (ansible_os_family in ['RedHat', 'AlmaLinux', 'Rocky'] and ansible_distribution_major_version | int == 8) or
        ansible_kernel.split('-')[0] is version('4.19.0', '>=')
  tags: worker_mod

- name: "Install iptables-nft"
  package:
    name: iptables-nft
    state: present
  when:
    - ansible_os_family in ['RedHat', 'AlmaLinux', 'Rocky', 'TencentOS']
    - kubernetes.kube_proxy_mode == 'ipvs'
  tags:
    - kube-proxy

- name: "Modprobe Kernel Module for nftables"
  modprobe:
    name: "nf_tables"
    state: present
    persistent: present
  when: kubernetes.kube_proxy_mode == 'nftables'
  tags:
    - kube-proxy

- name: "Set sysctl k8s parameters"
  ansible.posix.sysctl:
    sysctl_file: "/etc/sysctl.d/99-k8s.conf"
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    sysctl_set: true
    state: present
    reload: yes
  with_items:
    - {name: 'net.netfilter.nf_conntrack_max',value: '2310720' }
    - {name: 'net.bridge.bridge-nf-call-iptables',value: '1' }
    - {name: 'net.bridge.bridge-nf-call-ip6tables',value: '1' }
    - {name: 'net.bridge.bridge-nf-call-arptables',value: '1' }
    - {name: 'net.core.somaxconn',value: '32768' }
    - {name: 'net.core.wmem_max',value: '16777216' }
    - {name: 'net.core.rmem_max',value: '16777216' }
    - {name: 'net.core.netdev_max_backlog',value: '16384' }
    - {name: 'vm.max_map_count',value: '262144' }
    - {name: 'kernel.softlockup_panic',value: '1' }
    - {name: 'kernel.pid_max',value: '4194303' }
    - {name: 'kernel.softlockup_all_cpu_backtrace',value: '1' }
    - {name: 'net.ipv4.neigh.default.gc_thresh2',value: '1024' }
    - {name: 'net.ipv4.neigh.default.gc_thresh3',value: '8192' }
    - {name: 'net.ipv4.tcp_max_syn_backlog',value: '8096' }
    - {name: 'net.ipv4.tcp_wmem',value: '4096 12582912 16777216' }
    - {name: 'net.ipv4.tcp_rmem',value: '4096 12582912 16777216' }
    - {name: 'fs.file-max',value: '2097152' }
    - {name: 'fs.inotify.max_user_watches',value: '524288' }
    - {name: 'fs.inotify.max_queued_events',value: '16384' }
    - {name: 'fs.inotify.max_user_instances',value: '16384' }
    - {name: 'user.max_user_namespaces',value: '0' }
  tags:
    - sysctl
    - k8s_sysctl
    - worker_sysctl

- name: "Create kubernetes directory"
  file:
    path: "{{ item.line }}"
    owner: root
    group: root
    mode: 0755
    state: directory
  with_items:
    - {line: '/etc/kubernetes/pki'}
    - {line: '/etc/kubernetes/manifests'}
    - {line: "{{ kubernetes.kubelet_dir }}"}
  tags: dir

- name: "Create kubelet directory"
  file:
    path: "/etc/sysconfig"
    owner: root
    group: root
    mode: 0755
    state: directory
  when:
    - ansible_os_family in ["Debian"]
  tags: dir

- name: "Install worker"
  copy:
    src: "{{ download.dest }}/kubernetes/{{ kubernetes.version }}/{%- if ansible_architecture == 'x86_64' -%}amd64{%- else -%}arm64{%- endif -%}/{{ item }}"
    dest: "/usr/bin/"
    owner: root
    group: root
    mode: 0750
  with_items:
    - kubelet
    - kube-proxy
  tags: install_worker

- name: "Distribution worker certs"
  copy:
    src: "{{ item }}"
    dest: "/etc/kubernetes/pki/"
    owner: root
    group: root
    mode: 0644
  with_items:
    - "{{ cert.dir }}/ca.pem"
    - "{{ cert.dir }}/{{ ansible_hostname.split('-')[-2:] | join('-') | lower }}/kube-proxy.pem"
    - "{{ cert.dir }}/{{ ansible_hostname.split('-')[-2:] | join('-') | lower }}/kube-proxy.key"
  tags:
    - dis_worker_certs
    - dis_certs

- name: "Get bootstrap-token-id"
  shell: "cat {{ cert.dir }}/token | grep -v '^#' | awk -F '.' '{print $1}'"
  register: token_id
  connection: local
  tags:
    - get_token_id
    - bootstrap_token

- name: "Get bootstrap-token-secret"
  shell: "cat {{ cert.dir }}/token | grep -v '^#' | awk -F '.' '{print $2}'"
  register: token_secret
  connection: local
  tags:
    - get_token_secret
    - bootstrap_token

- name: "Distribution worker kubeconfig"
  template:
    src: "{{ item.src }}"
    owner: root
    group: root
    mode: 0640
    dest: "{{ item.dest }}"
  with_items:
    - {src: "bootstrap.kubeconfig.j2",dest: "/etc/kubernetes/bootstrap.kubeconfig" }
    - {src: "proxy.kubeconfig.j2",dest: "/etc/kubernetes/proxy.kubeconfig" }
  tags: dis_worker_kubeconfig

- name: "Distribution worker config"
  template:
    src: "{{ item.src }}"
    owner: root
    group: root
    mode: 0640
    dest: "{{ item.dest }}"
  with_items:
    - {src: "kubelet.conf.j2",dest: "/etc/kubernetes/kubelet.conf" }
    - {src: "10-kubelet.conf.j2",dest: "/etc/sysconfig/kubelet" }
    - {src: "kube-proxy.conf.j2",dest: "/etc/kubernetes/kube-proxy.conf" }
  tags: dis_worker_config

- name: "Distribution worker system unit"
  template:
    src: "{{ item.src }}"
    owner: root
    group: root
    mode: 0644
    dest: "{{ item.dest }}"
  with_items:
    - {src: "kubelet.service.j2",dest: "/usr/lib/systemd/system/kubelet.service" }
    - {src: "kube-proxy.service.j2",dest: "/usr/lib/systemd/system/kube-proxy.service" }
  tags: dis_worker_systemd

- name: "Check if bootstrap-token exists"
  shell: kubectl -n kube-system get secret bootstrap-token-{{ token_id.stdout }}
  ignore_errors: true
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true
  register: bootstrap_token_ready
  tags: bootstrap

- name: "Create bootstrap-token secret"
  when: bootstrap_token_ready.rc != 0
  shell: kubectl -n kube-system create secret generic bootstrap-token-{{ token_id.stdout }} --type 'bootstrap.kubernetes.io/token' --from-literal description="cluster bootstrap token" --from-literal token-id={{ token_id.stdout }} --from-literal token-secret={{ token_secret.stdout }} --from-literal usage-bootstrap-authentication=true --from-literal usage-bootstrap-signing=true --from-literal auth-extra-groups="system:bootstrappers:default-node-token,system:bootstrappers:worker,system:bootstrappers:ingress"
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true
  tags: bootstrap

- name: "Check if clusterrolebinding kubelet-bootstrap exists"
  shell: kubectl get clusterrolebinding kubelet-bootstrap
  ignore_errors: true
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true
  register: kubelet_bootstrap_ready
  tags: bootstrap

- name: "Create clusterrolebinding kubelet-bootstrap"
  when: kubelet_bootstrap_ready.rc != 0
  shell: kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers:default-node-token
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true
  tags: bootstrap

- name: "Check if node-autoapprove-bootstrap exists"
  shell: kubectl get clusterrolebinding node-autoapprove-bootstrap
  ignore_errors: true
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true
  register: node_autoapprove_bootstrap_ready
  tags: bootstrap

- name: "Create clusterrolebinding node-autoapprove-bootstrap"
  when: node_autoapprove_bootstrap_ready.rc != 0
  shell: kubectl create clusterrolebinding node-autoapprove-bootstrap --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers:default-node-token
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true
  tags: bootstrap

- name: "Check if clusterrolebinding node-autoapprove-certificate-rotation exists"
  shell: kubectl get clusterrolebinding node-autoapprove-certificate-rotation
  ignore_errors: true
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true
  register: node_autoapprove_certificate_rotation_ready
  tags: bootstrap

- name: "Create clusterrolebinding node-autoapprove-certificate-rotation"
  when: node_autoapprove_certificate_rotation_ready.rc != 0
  shell: kubectl create clusterrolebinding node-autoapprove-certificate-rotation --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes
  delegate_to: "{{ groups['master'][0] }}"
  run_once: true
  tags: bootstrap

- name: "Restart kubelet"
  systemd:
    name: kubelet
    state: restarted
    daemon_reload: yes
    enabled: yes
    masked: false
  tags: restart_kubelet

- name: "Waiting kubelet starting"
  wait_for:
    host: "{%- if kubernetes.ipv4_stack and kubernetes.ipv6_stack -%}{{ ansible_default_ipv4.address }}{%- elif kubernetes.ipv4_stack -%}{{ ansible_default_ipv4.address }}{%- else -%}{{ ansible_default_ipv6.address }}{%- endif -%}"
    port: 10250
    delay: 5
    sleep: 2
  tags: healthcheck

- name: "kubelet health check"
  uri:
    url: "http://{%- if kubernetes.ipv4_stack and kubernetes.ipv6_stack -%}{{ ansible_default_ipv4.address }}{%- elif kubernetes.ipv4_stack -%}{{ ansible_default_ipv4.address }}{%- else -%}[{{ ansible_default_ipv6.address }}]{%- endif -%}:10248/healthz"
    return_content: yes
    validate_certs: no
  register: kubelet
  failed_when: "'ok' not in kubelet.content"
  tags: healthcheck

- name: "Restart kube-proxy"
  systemd:
    name: kube-proxy
    state: restarted
    daemon_reload: yes
    enabled: yes
    masked: false
  tags: restart_proxy

- name: "Waiting kube-proxy starting"
  wait_for:
    host: "{%- if kubernetes.ipv4_stack and kubernetes.ipv6_stack -%}{{ ansible_default_ipv4.address }}{%- elif kubernetes.ipv4_stack -%}{{ ansible_default_ipv4.address }}{%- else -%}{{ ansible_default_ipv6.address }}{%- endif -%}"
    port: 10256
    delay: 5
    sleep: 2
  tags: healthcheck

- name: "kube-proxy health check"
  uri:
    url: "http://{%- if kubernetes.ipv4_stack and kubernetes.ipv6_stack -%}{{ ansible_default_ipv4.address }}{%- elif kubernetes.ipv4_stack -%}{{ ansible_default_ipv4.address }}{%- else -%}[{{ ansible_default_ipv6.address }}]{%- endif -%}:10256/healthz"
    return_content: yes
    validate_certs: no
    status_code: 200
  register: proxy
  tags: healthcheck

- name: "Check whether the master is ready?"
  shell: "kubectl get node | grep {{ hostvars[inventory_hostname]['hostname'].split('-')[-2:] | join('-') | lower }}"
  register: master_status
  until: '"Ready" in master_status.stdout'
  retries: 5
  delay: 3
  delegate_to: "{{ groups['master'][0] }}"
  when: inventory_hostname in groups['master']
  tags:
    - create_master_label
    - create_master_taint
    - create_label

- name: "Create taint for control-plane"
  shell: "kubectl taint nodes {{ hostvars[inventory_hostname]['hostname'].split('-')[-2:] | join('-') | lower }} node-role.kubernetes.io/control-plane=:NoSchedule --overwrite"
  delegate_to: "{{ groups['master'][0] }}"
  when: inventory_hostname in groups['master']
  tags:
    - create_master_label
    - create_master_taint
    - create_label

- name: "Create label for master"
  shell: "kubectl label nodes {{ hostvars[inventory_hostname]['hostname'].split('-')[-2:] | join('-') | lower }} node-role.kubernetes.io/control-plane= --overwrite"
  delegate_to: "{{ groups['master'][0] }}"
  when: inventory_hostname in groups['master']
  tags:
    - create_master_label
    - create_label

- name: "Check whether the worker is ready?"
  shell: "kubectl get node | grep {{ hostvars[inventory_hostname]['hostname'].split('-')[-2:] | join('-') | lower }}"
  register: worker_status
  until: '"Ready" in worker_status.stdout'
  retries: 5
  delay: 3
  delegate_to: "{{ groups['master'][0] }}"
  when: inventory_hostname in groups['worker']
  tags:
    - create_worker_label
    - create_label

- name: "Create label for worker"
  shell: "kubectl label nodes {{ hostvars[inventory_hostname]['hostname'].split('-')[-2:] | join('-') | lower }} node-role.kubernetes.io/worker= --overwrite"
  delegate_to: "{{ groups['master'][0] }}"
  when: inventory_hostname in groups['worker']
  tags:
    - create_worker_label
    - create_label

- name: "Create taint for gpu worker"
  when: gpu is defined and gpu == 'true'
  shell: "kubectl taint nodes {{ hostvars[inventory_hostname]['hostname'].split('-')[-2:] | join('-') | lower }} nvidia.com/gpu=:NoSchedule --overwrite"
  delegate_to: "{{ groups['master'][0] }}"
  tags:
    - create_gpu_taint
    - create_gpu

- name: "Create label for gpu worker"
  when: gpu is defined and gpu == 'true'
  shell: "kubectl label nodes {{ hostvars[inventory_hostname]['hostname'].split('-')[-2:] | join('-') | lower }} nvidia.com/gpu=true --overwrite"
  delegate_to: "{{ groups['master'][0] }}"
  tags:
    - create_gpu_label
    - create_gpu
